# -*- coding: utf-8 -*-
"""test_models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10n-7DQs5r4bM38XIVjPIETSk7oErI51K
"""

import torch
import numpy as np
from sentence_transformers import SentenceTransformer, util
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import warnings
warnings.filterwarnings('ignore')

def test1_sentence_transformers():
    """Тест 1: Сравнение сходства текстов с помощью Sentence Transformers"""
    print("=" * 60)
    print("ТЕСТ 1: Сравнение сходства текстов")
    print("=" * 60)

    # Загружаем модель Sentence Transformers
    model = SentenceTransformer('all-MiniLM-L6-v2')

    # Тексты для сравнения
    text1 = "Corrosion resistance of steel alloys"
    text2 = "Metallurgy advances in steel production"

    print(f"Текст 1: {text1}")
    print(f"Текст 2: {text2}")
    print()

    # Кодируем тексты
    embeddings1 = model.encode(text1, convert_to_tensor=True)
    embeddings2 = model.encode(text2, convert_to_tensor=True)

    # Вычисляем косинусное сходство
    cosine_similarity = util.cos_sim(embeddings1, embeddings2)

    print(f"Косинусное сходство: {cosine_similarity.item():.4f}")

    # Интерпретация результата
    similarity_score = cosine_similarity.item()
    if similarity_score >= 0.8:
        interpretation = "Очень высокое сходство"
    elif similarity_score >= 0.6:
        interpretation = "Высокое сходство"
    elif similarity_score >= 0.4:
        interpretation = "Умеренное сходство"
    elif similarity_score >= 0.2:
        interpretation = "Слабое сходство"
    else:
        interpretation = "Очень слабое сходство"

    print(f"Интерпретация: {interpretation}")
    print()

def test2_llama_generation():
    """Тест 2: Генерация гипотезы с помощью языковой модели"""
    print("=" * 60)
    print("ТЕСТ 2: Генерация гипотезы")
    print("=" * 60)

    # Используем доступную модель (GPT-2, так как LLaMA требует больше ресурсов)
    model_name = "gpt2"

    try:
        # Создаем пайплайн для генерации текста
        generator = pipeline(
            "text-generation",
            model=model_name,
            tokenizer=model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device=0 if torch.cuda.is_available() else -1
        )

        # Темы для генерации гипотезы
        topic1 = "steel corrosion"
        topic2 = "transformer architecture"

        print(f"Тема 1: {topic1}")
        print(f"Тема 2: {topic2}")
        print()

        # Промпт для генерации гипотезы
        prompt = f"""Based on the topics '{topic1}' and '{topic2}', generate a scientific hypothesis that connects these two fields:

Hypothesis:"""

        # Генерируем текст
        results = generator(
            prompt,
            max_length=200,
            num_return_sequences=1,
            temperature=0.8,
            do_sample=True,
            pad_token_id=50256
        )

        generated_text = results[0]['generated_text']

        # Извлекаем только сгенерированную часть (после промпта)
        hypothesis = generated_text.replace(prompt, "").strip()

        print("Сгенерированная гипотеза:")
        print(hypothesis)

    except Exception as e:
        print(f"Ошибка при загрузке GPT-2: {e}")
        print("Пытаемся использовать более легкую модель...")

        # Альтернативная модель
        try:
            generator = pipeline("text-generation", model="distilgpt2")

            topic1 = "steel corrosion"
            topic2 = "transformer architecture"

            print(f"Тема 1: {topic1}")
            print(f"Тема 2: {topic2}")
            print()

            prompt = f"Generate a hypothesis connecting {topic1} and {topic2}:"

            results = generator(
                prompt,
                max_length=150,
                num_return_sequences=1,
                temperature=0.9
            )

            hypothesis = results[0]['generated_text'].replace(prompt, "").strip()

            print("Сгенерированная гипотеза:")
            print(hypothesis)

        except Exception as e2:
            print(f"Ошибка при использовании альтернативной модели: {e2}")
            print("Генерация гипотезы не удалась.")

def additional_test_similarity():
    """Дополнительный тест с другими текстами"""
    print("\n" + "=" * 60)
    print("ДОПОЛНИТЕЛЬНЫЙ ТЕСТ: Сравнение разных текстов")
    print("=" * 60)

    model = SentenceTransformer('all-MiniLM-L6-v2')

    # Разные пары текстов для сравнения
    test_pairs = [
        ("Machine learning algorithms", "Artificial intelligence development"),
        ("Renewable energy sources", "Solar power technology"),
        ("Quantum computing", "Classical mechanics"),
        ("Climate change", "Weather patterns"),
    ]

    for text1, text2 in test_pairs:
        embeddings1 = model.encode(text1, convert_to_tensor=True)
        embeddings2 = model.encode(text2, convert_to_tensor=True)
        cosine_similarity = util.cos_sim(embeddings1, embeddings2)

        print(f"'{text1}' vs '{text2}'")
        print(f"Сходство: {cosine_similarity.item():.4f}")
        print()

if __name__ == "__main__":
    print("Запуск тестов с использованием Sentence Transformers и GPT-2")
    print("Это может занять несколько секунд...\n")

    # Тест 1: Сравнение сходства текстов
    test1_sentence_transformers()

    # Тест 2: Генерация гипотезы
    test2_llama_generation()

    # Дополнительный тест
    additional_test_similarity()